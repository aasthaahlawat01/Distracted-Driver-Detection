{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mikoaro/distracteddriver\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "lmjus2Gh0S1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4745948f-3d76-45dc-b96f-94e2a87bbaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mikoaro/distracteddriver?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.01G/4.01G [02:19<00:00, 30.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.models as models\n"
      ],
      "metadata": {
        "id": "EzwKVmIqOd8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Y40ie_8kOh2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/train'\n",
        "test_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/test'"
      ],
      "metadata": {
        "id": "I1ghFSQONc_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "TRAIN_DIR = train_path\n",
        "TEST_DIR = test_path\n",
        "split_ratio=0.2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "X1ccprjYQQBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),  # 480x480\n",
        "    transforms.RandomHorizontalFlip(),  #for increasing diversity\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),      #converting to tensor data\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "PsLILBj7jz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n"
      ],
      "metadata": {
        "id": "Dv61NaakQZQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)"
      ],
      "metadata": {
        "id": "x0QVZNn2TBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOubrs2kkhOT",
        "outputId": "86f592ed-fa5d-4d3a-ade3-0c4b3e5dcb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILwUWi7Pkjwb",
        "outputId": "a5bea37a-bfdc-45c7-fa26-989fc73112ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'c0': 0,\n",
              " 'c1': 1,\n",
              " 'c2': 2,\n",
              " 'c3': 3,\n",
              " 'c4': 4,\n",
              " 'c5': 5,\n",
              " 'c6': 6,\n",
              " 'c7': 7,\n",
              " 'c8': 8,\n",
              " 'c9': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "K5L6OyVqklba",
        "outputId": "42cd8017-5582-41e8-9de6-559643bee507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.folder.ImageFolder"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.folder.ImageFolder</b><br/>def __init__(root: Union[str, Path], transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, loader: Callable[[str], Any]=default_loader, is_valid_file: Optional[Callable[[str], bool]]=None, allow_empty: bool=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py</a>A generic data loader where the images are arranged in this way by default: ::\n",
              "\n",
              "    root/dog/xxx.png\n",
              "    root/dog/xxy.png\n",
              "    root/dog/[...]/xxz.png\n",
              "\n",
              "    root/cat/123.png\n",
              "    root/cat/nsdf3.png\n",
              "    root/cat/[...]/asd932_.png\n",
              "\n",
              "This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
              "the same methods can be overridden to customize the dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory path.\n",
              "    transform (callable, optional): A function/transform that takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.\n",
              "    loader (callable, optional): A function to load an image given its path.\n",
              "    is_valid_file (callable, optional): A function that takes path of an Image file\n",
              "        and check if the file is a valid file (used to check of corrupt files)\n",
              "    allow_empty(bool, optional): If True, empty folders are considered to be valid classes.\n",
              "        An error is raised on empty folders if False (default).\n",
              "\n",
              " Attributes:\n",
              "    classes (list): List of the class names sorted alphabetically.\n",
              "    class_to_idx (dict): Dict with items (class_name, class_index).\n",
              "    imgs (list): List of (image path, class_index) tuples</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 287);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huoxAAZRko2O",
        "outputId": "00d100dd-1ec9-4b6d-f2ca-c21f6027fcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG8P7pyNksj9",
        "outputId": "e57b0147-9af1-443c-e4bd-5162eff04ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22424"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_total = len(images)\n",
        "num_val = int(split_ratio* num_total)\n",
        "num_train = num_total - num_val\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, test_data = random_split(images, [num_train, num_val])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L-f2ygstoB68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5zZMZ-fofp1",
        "outputId": "2b3608b6-dd8a-474c-b066-2ccb095fb7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 17940\n",
            "Number of validation samples: 4484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "4_sTmqy5oods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw5ZvIkIyGnD",
        "outputId": "9216c18d-b5b3-49e8-e8b2-882dd77e81f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBNTA4qHyO7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=2, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        #setting the model to training mode\n",
        "        model.train()\n",
        "        #loss of current epoch\n",
        "        curr_loss = 0\n",
        "        correct_pred=0\n",
        "        total_pred=0\n",
        "\n",
        "        for images, classes in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            #clearing the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, classes )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            curr_loss += loss.item()\n",
        "\n",
        "            #the maximum value along the label dimension\n",
        "            value, predicted = torch.max(outputs, 1)\n",
        "            total_pred += classes .size(0)\n",
        "            correct_pred += (predicted == classes).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct_pred / total_pred\n",
        "        avg_loss=curr_loss/len(train_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss}, Accuracy: {train_accuracy}')\n",
        "\n",
        "        # evaluation mode\n",
        "        model.eval()\n",
        "        #disable grad computation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            correct_test_pred=0\n",
        "            total_test_pred=0\n",
        "            test_loss = 0\n",
        "\n",
        "            for images, classes in test_loader:\n",
        "                outputs = model(images)\n",
        "                test_loss += criterion(outputs, classes).item()\n",
        "                value, predicted = torch.max(outputs, 1)\n",
        "                total_test_pred += classes.size(0)\n",
        "                correct_test_pred += (predicted == classes).sum().item()\n",
        "\n",
        "            test_accuracy = 100 * correct_test_pred / total_test_pred\n",
        "\n",
        "            print(f'Validation Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "d5lbS-uqvlAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model,train_loader,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy6oYO-AzqW4",
        "outputId": "9383bed8-9c3d-4d67-933c-d156b824b50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.5216503134810776, Accuracy: 83.13823857302118\n",
            "Validation Accuracy: 93.53256021409456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████▏ | 456/561 [3:05:22<42:09, 24.09s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'distracted_driver_model2.pth')\n"
      ],
      "metadata": {
        "id": "A-_PYIPSRB0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model.load_state_dict(torch.load('distracted_driver_model2.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Predict on a single image\n",
        "from PIL import Image\n",
        "\n",
        "def predict(image_path, model, transform):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "image_path = '/test_image.jpeg'\n",
        "class_idx = predict(image_path, model, test_transform)\n",
        "class_name = images.classes[class_idx]\n",
        "print(f\"Predicted Class: {class_name}\")\n"
      ],
      "metadata": {
        "id": "kyISPS-HRG0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}