{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmjus2Gh0S1O",
        "outputId": "c6c1e3d9-742b-4ca0-f9e8-abe3c35233a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mikoaro/distracteddriver?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.01G/4.01G [00:48<00:00, 88.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mikoaro/distracteddriver\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzwKVmIqOd8h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import random_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y40ie_8kOh2-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1ghFSQONc_x"
      },
      "outputs": [],
      "source": [
        "train_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/train'\n",
        "test_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ccprjYQQBe"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "TRAIN_DIR = train_path\n",
        "TEST_DIR = test_path\n",
        "split_ratio=0.2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsLILBj7jz_M"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),  # 480x480\n",
        "    transforms.RandomHorizontalFlip(),  #for increasing diversity\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),      #converting to tensor data\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv61NaakQZQS"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0QVZNn2TBxl"
      },
      "outputs": [],
      "source": [
        "images = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOubrs2kkhOT",
        "outputId": "0657d270-6abe-42c2-d07e-1b0cd219cf09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILwUWi7Pkjwb",
        "outputId": "d0530c7c-82cb-4a35-f54f-f28c4e70ac17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'c0': 0,\n",
              " 'c1': 1,\n",
              " 'c2': 2,\n",
              " 'c3': 3,\n",
              " 'c4': 4,\n",
              " 'c5': 5,\n",
              " 'c6': 6,\n",
              " 'c7': 7,\n",
              " 'c8': 8,\n",
              " 'c9': 9}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "K5L6OyVqklba",
        "outputId": "459f5676-8e7e-497c-9ea2-298aff3266a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.folder.ImageFolder</b><br/>def __init__(root: Union[str, Path], transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, loader: Callable[[str], Any]=default_loader, is_valid_file: Optional[Callable[[str], bool]]=None, allow_empty: bool=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py</a>A generic data loader where the images are arranged in this way by default: ::\n",
              "\n",
              "    root/dog/xxx.png\n",
              "    root/dog/xxy.png\n",
              "    root/dog/[...]/xxz.png\n",
              "\n",
              "    root/cat/123.png\n",
              "    root/cat/nsdf3.png\n",
              "    root/cat/[...]/asd932_.png\n",
              "\n",
              "This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
              "the same methods can be overridden to customize the dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory path.\n",
              "    transform (callable, optional): A function/transform that takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.\n",
              "    loader (callable, optional): A function to load an image given its path.\n",
              "    is_valid_file (callable, optional): A function that takes path of an Image file\n",
              "        and check if the file is a valid file (used to check of corrupt files)\n",
              "    allow_empty(bool, optional): If True, empty folders are considered to be valid classes.\n",
              "        An error is raised on empty folders if False (default).\n",
              "\n",
              " Attributes:\n",
              "    classes (list): List of the class names sorted alphabetically.\n",
              "    class_to_idx (dict): Dict with items (class_name, class_index).\n",
              "    imgs (list): List of (image path, class_index) tuples</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 287);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "torchvision.datasets.folder.ImageFolder"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huoxAAZRko2O",
        "outputId": "3661d43f-7922-4152-a804-949503cd65d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
              " 0)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG8P7pyNksj9",
        "outputId": "2e48604b-4e73-4f04-8c31-a5437e2832ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22424"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-f2ygstoB68"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_total = len(images)\n",
        "num_val = int(split_ratio* num_total)\n",
        "num_train = num_total - num_val\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, test_data = random_split(images, [num_train, num_val])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5zZMZ-fofp1",
        "outputId": "8640d3c5-b77a-471b-e14d-5a476db51e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 17940\n",
            "Number of validation samples: 4484\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_sTmqy5oods"
      },
      "outputs": [],
      "source": [
        "# DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjNmslLDxj-O"
      },
      "outputs": [],
      "source": [
        "def weights_init(model, type_of_init='zero'):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            if type_of_init == 'zero':\n",
        "                nn.init.constant_(layer.weight, 0)\n",
        "            elif type_of_init == 'random':\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "            elif type_of_init == 'he':\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4R-2Kpk07v"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,dropout_prob=0.2, active_fun='RELU'):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # activation function\n",
        "        if active_fun == 'TANH':\n",
        "            self.active_fun = nn.Tanh()\n",
        "        elif active_fun == 'RELU':\n",
        "            self.active_fun = nn.ReLU()\n",
        "\n",
        "        self.last_lay_activation = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "        self.block_1 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max ooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block_2 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max ooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block_3 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max Pooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.dropout_conv = nn.Dropout(dropout_prob)\n",
        "        self.dropout_fc = nn.Dropout(dropout_prob)\n",
        "        # Fully Connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            self.dropout_fc,\n",
        "            nn.Linear(32 * (IMG_HEIGHT // 2) * (IMG_WIDTH // 2), 128),\n",
        "            self.active_fun,\n",
        "            self.dropout_fc,\n",
        "            nn.Linear(128, 10),\n",
        "            self.last_lay_activation\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        #x = self.block_2(x)\n",
        "        #x = self.block_3(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYx04KiTQmY1"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(DEVICE)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5lbS-uqvlAU"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        #setting the model to training mode\n",
        "        model.train()\n",
        "        #loss of current epoch\n",
        "        curr_loss = 0\n",
        "        correct_pred=0\n",
        "        total_pred=0\n",
        "\n",
        "        for images, classes in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            #clearing the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, classes )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            curr_loss += loss.item()\n",
        "\n",
        "            #the maximum value along the label dimension\n",
        "            value, predicted = torch.max(outputs, 1)\n",
        "            total_pred += classes .size(0)\n",
        "            correct_pred += (predicted == classes).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct_pred / total_pred\n",
        "        avg_loss=curr_loss/len(train_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss}, Accuracy: {train_accuracy}')\n",
        "\n",
        "        # evaluation mode\n",
        "        model.eval()\n",
        "        #disable grad computation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            correct_test_pred=0\n",
        "            total_test_pred=0\n",
        "            test_loss = 0\n",
        "\n",
        "            for images, classes in test_loader:\n",
        "                outputs = model(images)\n",
        "                test_loss += criterion(outputs, classes).item()\n",
        "                value, predicted = torch.max(outputs, 1)\n",
        "                total_test_pred += classes.size(0)\n",
        "                correct_test_pred += (predicted == classes).sum().item()\n",
        "\n",
        "            test_accuracy = 100 * correct_test_pred / total_test_pred\n",
        "\n",
        "            print(f'Validation Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA_RMe5Ru3en",
        "outputId": "72758c06-79c4-4f47-cdd4-adb0f7575b27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 2.358081903474641, Accuracy: 10.289855072463768\n",
            "Validation Accuracy: 10.437109723461194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Average Loss: 2.3590884654917184, Accuracy: 10.206243032329988\n",
            "Validation Accuracy: 10.905441570026762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Average Loss: 2.3592567065607843, Accuracy: 10.189520624303233\n",
            "Validation Accuracy: 10.905441570026762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|█         | 59/561 [03:53<32:37,  3.90s/it]"
          ]
        }
      ],
      "source": [
        "train_model(model,train_loader,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-_PYIPSRB0O"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'distracted_driver_model4.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XSFbVQzvzML"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyISPS-HRG0w"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model.load_state_dict(torch.load('distracted_driver_model4.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Predict on a single image\n",
        "from PIL import Image\n",
        "\n",
        "def predict(image_path, model, transform):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "image_path = '/test_image.jpeg'\n",
        "class_idx = predict(image_path, model, test_transform)\n",
        "class_name = images.classes[class_idx]\n",
        "print(f\"Predicted Class: {class_name}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}