{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mikoaro/distracteddriver\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "lmjus2Gh0S1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1f6e0d-a161-47f5-b8c3-85cfa00dc4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import random_split\n"
      ],
      "metadata": {
        "id": "EzwKVmIqOd8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Y40ie_8kOh2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/train'\n",
        "test_path='/root/.cache/kagglehub/datasets/mikoaro/distracteddriver/versions/1/distracted-driver-detection/test'"
      ],
      "metadata": {
        "id": "I1ghFSQONc_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "IMG_HEIGHT = 480\n",
        "IMG_WIDTH = 480\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "TRAIN_DIR = train_path\n",
        "TEST_DIR = test_path\n",
        "split_ratio=0.2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "X1ccprjYQQBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.RandomHorizontalFlip(),  #for increasing diversity\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),      #converting to tensor data\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "PsLILBj7jz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n"
      ],
      "metadata": {
        "id": "Dv61NaakQZQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)"
      ],
      "metadata": {
        "id": "x0QVZNn2TBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOubrs2kkhOT",
        "outputId": "58934aac-0877-4f9a-f4a2-ad68d457bde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILwUWi7Pkjwb",
        "outputId": "8322b922-72ed-4f46-d04f-f9b9227b253b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'c0': 0,\n",
              " 'c1': 1,\n",
              " 'c2': 2,\n",
              " 'c3': 3,\n",
              " 'c4': 4,\n",
              " 'c5': 5,\n",
              " 'c6': 6,\n",
              " 'c7': 7,\n",
              " 'c8': 8,\n",
              " 'c9': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "K5L6OyVqklba",
        "outputId": "f49f415e-9f11-4746-81c6-3f68066a5393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.folder.ImageFolder"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.folder.ImageFolder</b><br/>def __init__(root: Union[str, Path], transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, loader: Callable[[str], Any]=default_loader, is_valid_file: Optional[Callable[[str], bool]]=None, allow_empty: bool=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py</a>A generic data loader where the images are arranged in this way by default: ::\n",
              "\n",
              "    root/dog/xxx.png\n",
              "    root/dog/xxy.png\n",
              "    root/dog/[...]/xxz.png\n",
              "\n",
              "    root/cat/123.png\n",
              "    root/cat/nsdf3.png\n",
              "    root/cat/[...]/asd932_.png\n",
              "\n",
              "This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
              "the same methods can be overridden to customize the dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory path.\n",
              "    transform (callable, optional): A function/transform that takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.\n",
              "    loader (callable, optional): A function to load an image given its path.\n",
              "    is_valid_file (callable, optional): A function that takes path of an Image file\n",
              "        and check if the file is a valid file (used to check of corrupt files)\n",
              "    allow_empty(bool, optional): If True, empty folders are considered to be valid classes.\n",
              "        An error is raised on empty folders if False (default).\n",
              "\n",
              " Attributes:\n",
              "    classes (list): List of the class names sorted alphabetically.\n",
              "    class_to_idx (dict): Dict with items (class_name, class_index).\n",
              "    imgs (list): List of (image path, class_index) tuples</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 287);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huoxAAZRko2O",
        "outputId": "3bca1ede-c031-4d32-9b53-0314c331c4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG8P7pyNksj9",
        "outputId": "68ee5655-d893-475b-933d-fc3e1c64ec44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22424"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_total = len(images)\n",
        "num_val = int(split_ratio* num_total)\n",
        "num_train = num_total - num_val\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, test_data = random_split(images, [num_train, num_val])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L-f2ygstoB68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5zZMZ-fofp1",
        "outputId": "24769c41-2438-45fe-ef3f-ddb3f801f611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 17940\n",
            "Number of validation samples: 4484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "4_sTmqy5oods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(model, type_of_init='zero'):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            if type_of_init == 'zero':\n",
        "                nn.init.constant_(layer.weight, 0)\n",
        "            elif type_of_init == 'random':\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "            elif type_of_init == 'he':\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
      ],
      "metadata": {
        "id": "JjNmslLDxj-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,dropout_prob=0.2, active_fun='RELU'):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # activation function\n",
        "        if active_fun == 'TANH':\n",
        "            self.active_fun = nn.Tanh()\n",
        "        elif active_fun == 'RELU':\n",
        "            self.active_fun = nn.ReLU()\n",
        "\n",
        "        self.last_lay_activation = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "        self.block_1 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max ooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block_2 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max ooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block_3 = nn.Sequential(\n",
        "            #convolutional layer\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            self.active_fun,\n",
        "            #max Pooling layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.dropout_conv = nn.Dropout(dropout_prob)\n",
        "        self.dropout_fc = nn.Dropout(dropout_prob)\n",
        "        # Fully Connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            self.dropout_fc,\n",
        "            nn.Linear(32 * (IMG_HEIGHT // 8) * (IMG_WIDTH // 8), 256),\n",
        "            self.active_fun,\n",
        "            self.dropout_fc,\n",
        "            nn.Linear(256, 10),\n",
        "            self.last_lay_activation\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        #x = self.block_2(x)\n",
        "        #x = self.block_3(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JQ4R-2Kpk07v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(DEVICE)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "KYx04KiTQmY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        #setting the model to training mode\n",
        "        model.train()\n",
        "        #loss of current epoch\n",
        "        curr_loss = 0\n",
        "        correct_pred=0\n",
        "        total_pred=0\n",
        "\n",
        "        for images, classes in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            #clearing the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, classes )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            curr_loss += loss.item()\n",
        "\n",
        "            #the maximum value along the label dimension\n",
        "            value, predicted = torch.max(outputs, 1)\n",
        "            total_pred += classes .size(0)\n",
        "            correct_pred += (predicted == classes).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct_pred / total_pred\n",
        "        avg_loss=curr_loss/len(train_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss}, Accuracy: {train_accuracy}')\n",
        "\n",
        "        # evaluation mode\n",
        "        model.eval()\n",
        "        #disable grad computation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            correct_test_pred=0\n",
        "            total_test_pred=0\n",
        "            test_loss = 0\n",
        "\n",
        "            for images, classes in test_loader:\n",
        "                outputs = model(images)\n",
        "                test_loss += criterion(outputs, classes).item()\n",
        "                value, predicted = torch.max(outputs, 1)\n",
        "                total_test_pred += classes.size(0)\n",
        "                correct_test_pred += (predicted == classes).sum().item()\n",
        "\n",
        "            test_accuracy = 100 * correct_test_pred / total_test_pred\n",
        "\n",
        "            print(f'Validation Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "d5lbS-uqvlAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model,train_loader,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA_RMe5Ru3en",
        "outputId": "b9890b0b-f73e-49d1-9100-05787070fcd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.09824329179026, Accuracy: 35.68561872909699\n",
            "Validation Accuracy: 45.45049063336307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 1.9560919373218175, Accuracy: 50.17837235228539\n",
            "Validation Accuracy: 58.38537020517395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 1.8681234788554661, Accuracy: 59.1025641025641\n",
            "Validation Accuracy: 63.60392506690455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 1.8020870487957714, Accuracy: 65.80267558528428\n",
            "Validation Accuracy: 69.7814451382694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 1.7647071732556756, Accuracy: 69.45373467112597\n",
            "Validation Accuracy: 74.06333630686886\n"
          ]
        }
      ]
    }
  ]
}